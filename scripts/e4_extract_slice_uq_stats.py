import SimpleITK as sitk
import numpy as np
import sqlite3
import pandas as pd
import logging

from datetime import datetime
from pathlib import Path
from typing import List, Tuple, Dict, Any
from scipy.stats import spearmanr

from assets.dicom_utils import resample_to_reference
from assets.image_operations import apply_gaussian_blur_3d


# """
# ================================================================================
# Uncertainty‚ÄìError Correlation Extraction for Prostate MRI
# ================================================================================

# This script computes and stores voxel-wise statistical metrics that quantify 
# the relationship between uncertainty maps (UQ) and absolute reconstruction error 
# in AI-accelerated prostate MRI reconstructions.

# --------------------------------------------------------------------------------
# üéØ Scientific Purpose
# --------------------------------------------------------------------------------
# To evaluate whether uncertainty maps generated by two different methods‚Äî
# Gaussian Monte Carlo (MC) and structured echo-train dropout (LXO)‚Äîcorrelate 
# with actual reconstruction error. Metrics are computed:
#     ‚Ä¢ Per-slice
#     ‚Ä¢ Per-region (whole slice, prostate, lesion)
#     ‚Ä¢ Per-acceleration factor (R=3, R=6)
#     ‚Ä¢ Per-method (Gaussian vs LXO)

# This supports reproducible, region-stratified correlation analysis of UQ quality.

# --------------------------------------------------------------------------------
# üìä Metrics Stored Per Region (and Slice)
# --------------------------------------------------------------------------------
# | Metric              | Description                                       |
# |---------------------|---------------------------------------------------|
# | mean_abs            | Mean of voxel-wise absolute error                |
# | mean_uq             | Mean of voxel-wise uncertainty (already CV)      |
# | std_uq              | Std of voxel-wise uncertainty (spread of CVs)    |
# | pearson_corr        | Pearson correlation between UQ and error maps    |
# | spearman_corr       | Spearman rank correlation (monotonicity)         |

# These are stored in long-format SQLite or DataFrame: one row per 
# slice √ó region √ó acceleration √ó UQ method.

# --------------------------------------------------------------------------------
# üîÅ Looping Structure (5 Levels)
# --------------------------------------------------------------------------------
# LEVEL 1 - for uq_method in uq_methods:           # Gaussian / LXO
# LEVEL 2 -   for pat_id in pat_ids:               # One patient at a time
# LEVEL 3 -     for acc_factor in acc_factors:     # R=3, R=6
# LEVEL 4 -       for slice_idx in num_slices:     # Axial slice
# LEVEL 5 -         for region in ['slice', 'prostate', 'lesion']:  
#                                                   # Region-wise metrics

# Each valid (slice, region) pair yields one long-format row of results.

# --------------------------------------------------------------------------------
# üíæ Output
# --------------------------------------------------------------------------------
# SQLite table (or DataFrame) with schema:

# | pat_id | slice_idx | acc_factor | uq_method | region | 
# |--------|------------|-------------|------------|--------|
# | mean_abs | mean_uq | std_uq | pearson_corr | spearman_corr |

# Images themselves are not stored‚Äîonly derived statistics.

# --------------------------------------------------------------------------------
# üîß Notes
# --------------------------------------------------------------------------------
# - UQ maps are precomputed voxel-wise CVs across sub-reconstructions.
# - Absolute error maps are computed as |recon - reference|.
# - Small or empty regions (<2 voxels) are excluded.
# - Optional Gaussian blurring can be applied to smooth maps before stat calc.

# """


def setup_logger(name="uq_logger") -> logging.Logger:
    logs_dir = Path("logs")
    logs_dir.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_path = logs_dir / f"log_{timestamp}.log"

    #lets add the filename of the file that created the log, so: e4_extract_slice_uq_stats.py
    log_path = log_path.with_name(f"{log_path.stem}_{name}.log")

    logger = logging.getLogger(name)
    logger.setLevel(logging.DEBUG)

    # File handler
    fh = logging.FileHandler(log_path)
    fh.setLevel(logging.DEBUG)
    fh.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))

    # Console handler
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch.setFormatter(logging.Formatter("%(message)s"))

    # Avoid duplicate handlers
    if not logger.handlers:
        logger.addHandler(fh)
        logger.addHandler(ch)

    return logger

# üëá Global access point
LOGGER = setup_logger()


def get_combined_rois_array(pat_root: Path, r1_ref_image: sitk.Image, r1_arr: np.ndarray) -> Tuple[np.ndarray, List[int]]:
    roi_fpaths = list(pat_root.glob("*_roi_*.mha"))
    roi_arrs_combined = np.zeros_like(r1_arr)

    # Check if any ROI files are found
    if len(roi_fpaths) == 0:
        LOGGER.warning(f"\tNo ROIs found in {pat_root.name}.")
        return roi_arrs_combined, []

    for roi_fpath in roi_fpaths:
        roi_img = sitk.ReadImage(str(roi_fpath))
        roi_img_resampled = resample_to_reference(roi_img, r1_ref_image)
        roi_arr = sitk.GetArrayFromImage(roi_img_resampled)
        roi_arrs_combined += roi_arr
    slice_idxs_lesion = [i for i in range(len(roi_arrs_combined)) if np.sum(roi_arrs_combined[i]) > 0]
    LOGGER.info(f"\tCombined ROI {roi_fpath.name} has {len(slice_idxs_lesion)} slices with lesions. With idxs: {slice_idxs_lesion}")
    return roi_arrs_combined, slice_idxs_lesion


def load_reference_and_masks(pat_id: str, roots: Dict[str, Path]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    pat_root = roots["reader_study"] / pat_id
    r1_img = sitk.ReadImage(str(pat_root / f"{pat_id}_rss_target_dcml.mha"))
    r1_arr = sitk.GetArrayFromImage(r1_img)

    roi_arr, _ = get_combined_rois_array(pat_root, r1_img, r1_arr)

    prost_seg = sitk.ReadImage(str(roots["reader_study_segs"] / f"{pat_id}_mlseg_total_mr.nii.gz"))
    prost_arr = (sitk.GetArrayFromImage(prost_seg) == 17).astype(np.uint8)

    return r1_arr, roi_arr, prost_arr


def load_abs_and_uq_maps(
    pat_id: str,
    roots: Dict[str, Path],
    acc: int,
    r1_arr: np.ndarray,
    do_blur: bool,
    uq_method: str = "gaussian",
    uq_metric: str = "std",
) -> Tuple[np.ndarray, np.ndarray]:
    
    recon_path = roots["reader_study"] / pat_id / f"{pat_id}_VSharp_R{acc}_recon_dcml.mha"
    recon_arr = sitk.GetArrayFromImage(sitk.ReadImage(str(recon_path)))
    abs_arr = np.abs(r1_arr - recon_arr)

    uq_path = roots[f"R{acc}"] / pat_id / f"uq_map_R{acc}_{uq_method}_{uq_metric}.nii.gz"
    uq_arr = sitk.GetArrayFromImage(sitk.ReadImage(str(uq_path)))

    if do_blur:
        abs_arr = apply_gaussian_blur_3d(abs_arr, sigma_xy=1.0, sigma_z=0.0)
        uq_arr = apply_gaussian_blur_3d(uq_arr, sigma_xy=1.0, sigma_z=0.0)

    return abs_arr, uq_arr


def compute_region_metrics(voxelwise_error: np.ndarray, voxelwise_uq_cv: np.ndarray) -> Dict[str, np.float32]:
    voxelwise_error = voxelwise_error.flatten()
    voxelwise_uq_cv = voxelwise_uq_cv.flatten()
    return {
        "mean_abs": np.mean(voxelwise_error),
        "mean_uq": np.mean(voxelwise_uq_cv),
        "std_uq": np.std(voxelwise_uq_cv),
        "pearson_corr": np.corrcoef(voxelwise_error, voxelwise_uq_cv)[0, 1],
        "spearman_corr": spearmanr(voxelwise_error, voxelwise_uq_cv).correlation
    }


def compute_slice_level_stats(
    pat_id: str,
    roots: Dict[str, Path],
    acc_factors: List[int],
    uq_method: str = "gaussian",
    uq_metric: str = "std",
    do_blurring: bool = False,
    debug: bool = False,
) -> List[Dict[str, Any]]:
    """
    Compute per-slice statistics for a patient across acceleration factors and regions.

    Returns:
        List[Dict[str, Any]]: long-format rows (one per slice √ó region √ó acc_factor)
    """
    LOGGER.info(f"\nüß† Processing patient: {pat_id}")
    r1_arr, roi_arr, prost_arr = load_reference_and_masks(pat_id, roots)
    # LOGGER.info(f"üìê Shape of reference image: {r1_arr.shape}")

    all_rows = []

    for acc in acc_factors:
        LOGGER.info(f"üöÄ Acceleration factor R={acc} with {uq_method}")
        abs_arr, uq_arr = load_abs_and_uq_maps(pat_id, roots, acc, r1_arr, do_blurring, uq_method, uq_metric)
        # LOGGER.info(f"  ‚û§ abs_arr shape: {abs_arr.shape}, uq_arr shape: {uq_arr.shape}")

        for i in range(r1_arr.shape[0]):
            regions = {
                "slice": None,
                "prostate": prost_arr[i] == 1,
                "lesion": roi_arr[i] > 0,
            }

            for region_name, mask in regions.items():
                if mask is not None and not np.any(mask):
                    LOGGER.warning(f"    ‚ö†Ô∏è Empty mask for region '{region_name}' in slice {i}")
                    continue

                voxelwise_error = abs_arr[i] if mask is None else abs_arr[i][mask]
                voxelwise_uq_cv = uq_arr[i] if mask is None else uq_arr[i][mask]

                if voxelwise_error.size < 2 or voxelwise_uq_cv.size < 2:
                    LOGGER.warning(f"    ‚õî Insufficient data in slice {i}, region '{region_name}'")
                    continue

                metrics = compute_region_metrics(voxelwise_error, voxelwise_uq_cv)

                row = {
                    "pat_id": pat_id,
                    "slice_idx": i,
                    "acc_factor": acc,
                    "uq_method": uq_method,
                    "region": region_name,
                    **metrics,
                }
                all_rows.append(row)

                LOGGER.info(f"    ‚úÖ Slice {i:02d}, Region: {region_name:8s} | mean_abs: {metrics['mean_abs']:.4f}, "
                        f"mean_uq(CV): {metrics['mean_uq']:.4f}, Pearson: {metrics['pearson_corr']:.3f}, spearman_corr: {metrics['spearman_corr']:.3f}")

    LOGGER.info(f"üìä Total rows for {pat_id}: {len(all_rows)}")
    return all_rows


def process_patients_and_store_stats(
    pat_ids: List[str]      = None,
    acc_factors: List[int]  = None,
    uq_method: str          = "gaussian",
    uq_metric: str          = "std",
    roots: Dict[str, Path]  = None,
    db_fpath: Path          = None,    
    table_name: str         = "uq_vs_abs_stats",
    do_blurring: bool       = False,
    debug: bool             = False,
):
    """
    Process multiple patients, compute slice-level statistics, and store them in a SQLite database.
    Skips patients already present in the database for the given UQ method.
    """
    all_stats = []
    processed_count = 0
    skipped_count = 0

    for idx, pat_id in enumerate(pat_ids):
        if patient_exists_in_table(db_fpath, table_name, pat_id, uq_method):
            LOGGER.info(f"‚è≠Ô∏è  {idx + 1}/{len(pat_ids)} Skipping patient {pat_id} (already in table for '{uq_method}')")
            skipped_count += 1
            continue

        LOGGER.info(f"üîÑ {idx + 1}/{len(pat_ids)} Processing patient {pat_id}...")
        patient_stats = compute_slice_level_stats(
            pat_id      = pat_id,
            roots       = roots,
            acc_factors = acc_factors,
            uq_method   = uq_method,
            uq_metric   = uq_metric,
            do_blurring = do_blurring,
            debug       = debug,
        )
        all_stats.extend(patient_stats)
        processed_count += 1

    if not all_stats:
        LOGGER.warning("‚ö†Ô∏è No new data computed. Exiting early.")
        return

    long_df = pd.DataFrame(all_stats)

    LOGGER.info(f"üì• Storing {processed_count} new patients in database ({skipped_count} skipped).")
    with sqlite3.connect(str(db_fpath)) as conn:
        long_df.to_sql(table_name, conn, if_exists="append", index=False)
    LOGGER.info(f"‚úÖ Appended to table '{table_name}' in {db_fpath}")


def create_table_if_not_exists(db_fpath: Path, table_name: str):
    """
    Create a long-format SQLite table to store per-slice UQ vs error statistics.

    Args:
        db_fpath (Path): Path to the SQLite database file.
        table_name (str): Base name of the table to be created.
        debug (bool): If True, appends '_debug' to the table name.

    Returns:
        str: Final table name used.
    """
    # Ensure database path exists
    if not db_fpath.exists():
        LOGGER.info(f"üìÇ Creating new database file at {db_fpath}")
        db_fpath.parent.mkdir(parents=True, exist_ok=True)
        db_fpath.touch()

    schema = f"""
    CREATE TABLE IF NOT EXISTS {table_name} (
        pat_id TEXT NOT NULL,
        slice_idx INTEGER NOT NULL,
        acc_factor INTEGER NOT NULL,
        uq_method TEXT NOT NULL,
        region TEXT NOT NULL,
        mean_abs REAL,
        mean_uq REAL,
        std_uq REAL,
        pearson_corr REAL,
        spearman_corr REAL
    );
    """
    try:
        with sqlite3.connect(str(db_fpath)) as conn:
            cursor = conn.cursor()
            cursor.execute(schema)
            conn.commit()

        LOGGER.info(f"‚úÖ Verified/created table '{table_name}' in {db_fpath.name}")
    except Exception as e:
        LOGGER.error(f"‚ùå Failed to create/check table '{table_name}': {e}")
        raise

    return table_name


def patient_exists_in_table(
    db_fpath: Path,
    table_name: str,
    pat_id: str,
    uq_method: str
) -> bool:
    """
    Check if a patient has already been processed for a given UQ method.

    Args:
        db_fpath (Path): Path to the SQLite database file.
        table_name (str): Name of the table to query.
        pat_id (str): Patient ID.
        uq_method (str): UQ method name (e.g., 'gaussian', 'lxo').

    Returns:
        bool: True if patient exists in table for this UQ method.
    """
    query = f"""
        SELECT 1 FROM {table_name}
        WHERE pat_id = ? AND uq_method = ?
        LIMIT 1;
    """
    try:
        with sqlite3.connect(str(db_fpath)) as conn:
            cursor = conn.cursor()
            cursor.execute(query, (pat_id, uq_method))
            result = cursor.fetchone()
        return result is not None
    except Exception as e:
        LOGGER.error(f"‚ùå Error checking patient {pat_id} in table {table_name}: {e}")
        return False



# -----------------------------------------------------------------------------
# Main execution block
if __name__ == '__main__':

    # Step 1: Initialize parameters and logging
    # Step 2: Define patient IDs, acceleration factors, UQ methods, and regions of interest
    # Step 3: Create or drop the SQLite table if it exists
    # Step 4: Loop through UQ methods, patients, and acceleration factors
    # Step 5: Compute slice-level statistics and store them in the database
    # step 6: Log the results and completion status
    # step 7: Exit gracefully

    # === Statistical Parameters for Uncertainty Quantification ===
    pat_ids     = [
        '0003_ANON5046358',
        '0004_ANON9616598',
        '0005_ANON8290811',
        '0006_ANON2379607',
        '0007_ANON1586301',
        '0008_ANON8890538',
        '0010_ANON7748752',
        '0011_ANON1102778',
        '0012_ANON4982869',
        '0013_ANON7362087',
        '0014_ANON3951049',
        '0015_ANON9844606',
        '0018_ANON9843837',
        '0019_ANON7657657',
        '0020_ANON1562419',
        '0021_ANON4277586',
        '0023_ANON6964611',
        '0024_ANON7992094',
        '0026_ANON3620419',
        '0027_ANON9724912',
        '0028_ANON3394777',
        '0029_ANON7189994',
        '0030_ANON3397001',
        '0031_ANON9141039',
        '0032_ANON7649583',
        '0033_ANON9728185',
        '0035_ANON3474225',
        '0036_ANON0282755',
        '0037_ANON0369080',
        '0039_ANON0604912',
        '0042_ANON9423619',
        '0043_ANON7041133',
        '0044_ANON8232550',
        '0045_ANON2563804',
        '0047_ANON3613611',
        '0048_ANON6365688',
        '0049_ANON9783006',
        '0051_ANON1327674',
        '0052_ANON9710044',
        '0053_ANON5517301',
        '0055_ANON3357872',
        '0056_ANON2124757',
        '0057_ANON1070291',
        '0058_ANON9719981',
        '0059_ANON7955208',
        '0061_ANON7642254',
        '0062_ANON0319974',
        '0063_ANON9972960',
        '0064_ANON0282398',
        '0067_ANON0913099',
        '0068_ANON7978458',
        '0069_ANON9840567',
        '0070_ANON5223499',
        '0071_ANON9806291',
        '0073_ANON5954143',
        '0075_ANON5895496',
        '0076_ANON3983890',
        '0077_ANON8634437',
        '0078_ANON6883869',
        '0079_ANON8828023',
        '0080_ANON4499321',
        '0081_ANON9763928',
        '0082_ANON6073234',
        '0083_ANON9898497',
        '0084_ANON6141178',
        '0085_ANON4535412',
        '0086_ANON8511628',
        '0087_ANON9534873',
        '0088_ANON9892116',
        '0089_ANON9786899',
        '0090_ANON0891692',
        '0092_ANON9941969',
        '0093_ANON9728761',
        '0094_ANON8024204',
        '0095_ANON4189062',
        '0097_ANON5642073',
        '0103_ANON8583296',
        '0104_ANON7748630',
        '0105_ANON9883201',
        '0107_ANON4035085',
        '0108_ANON0424679',
        '0109_ANON9816976',
        '0110_ANON8266491',
        '0111_ANON9310466',
        '0112_ANON3210850',
        '0113_ANON9665113',
        '0115_ANON0400743',
        '0116_ANON9223478',
        # '0118_ANON7141024',
        '0119_ANON3865800',
        '0120_ANON7275574',
        '0121_ANON9629161',
        '0123_ANON7265874',
        '0124_ANON8610762',
        '0125_ANON0272089',
        '0126_ANON4747182',
        '0127_ANON8023509',
        '0128_ANON8627051',
        '0129_ANON5344332',
        '0135_ANON9879440',
        '0136_ANON8096961',
        '0137_ANON8035619',
        '0138_ANON1747790',
        '0139_ANON2666319',
        '0140_ANON0899488',
        '0141_ANON8018038',
        '0142_ANON7090827',
        '0143_ANON9752849',
        '0144_ANON2255419',
        '0145_ANON0335209',
        '0146_ANON7414571',
        '0148_ANON9604223',
        '0149_ANON4712664',
        '0150_ANON5824292',
        '0152_ANON2411221',
        '0153_ANON5958718',
        '0155_ANON7828652',
        '0157_ANON9873056',
        '0159_ANON9720717',
        '0160_ANON3504149'
    ]
    acc_factors = [3, 6] # Define the set of acceleration factors we care about.
    uq_methods  = ["gaussian", "lxo"] # UQ methods to process (e.g., "gaussian", "lxo")
    regions     = ["slice", "prostate", "lesion"]  # Regions to compute stats for
    uq_metric   = "std"  # Metric to use for uncertainty quantification (e.g., "std" for standard deviation) 

    # === Configurable Params ===
    do_blurring = True
    verbose     = True
    debug       = False
    tablename   = f"uq_vs_error_correlation_{uq_metric}"  # Base name for the SQLite table to store results
    db_fpath    = Path('/home1/p290820/repos/Uncertainty-Quantification-Prostate-MRI/databases/master_habrok_20231106_v2.db')
    
    # === Calculated Params ===
    tablename = f"{tablename}_debug" if debug else tablename
    uq_metrics = {  
        'std': "std",
        'cv': "",
    }
    
    # === Initialization ===
    LOGGER.info(f"Parameters: do_blurring={do_blurring}, verbose={verbose}, debug={debug}, tablename={tablename}")
    LOGGER.info(f"Patient IDs: {pat_ids}")
    LOGGER.info(f"Acceleration Factors: {acc_factors}, UQ Methods: {uq_methods}, Regions: {regions}")

    # === Optional: Drop Table ===
    if debug:
        LOGGER.info(f"üß® Dropping table '{tablename}' if it exists...")
        with sqlite3.connect(str(db_fpath)) as conn:
            cursor = conn.cursor()
            cursor.execute(f"DROP TABLE IF EXISTS {tablename}")
            conn.commit()
        LOGGER.info(f"‚úÖ Table '{tablename}' dropped successfully.")
    
    # === Create Table ===
    create_table_if_not_exists(db_fpath, tablename)

    # === Level 1: UQ Method Loop ===
    for u_method in uq_methods:
        LOGGER.info(f"\nüîÅ Processing UQ Method: {u_method}")
        roots = {
            'reader_study':      Path('/scratch/hb-pca-rad/projects/03_reader_set_v2'),
            'reader_study_segs': Path('/scratch/hb-pca-rad/projects/03_reader_set_v2/segs'),
            'kspace_root':       Path('/scratch/p290820/datasets/003_umcg_pst_ksps'),
            'R3':                Path(f"/scratch/hb-pca-rad/projects/04_uncertainty_quantification/{u_method}/recons_{3}x"),    # This path we can set dynamically later in the code on another level.
            'R6':                Path(f"/scratch/hb-pca-rad/projects/04_uncertainty_quantification/{u_method}/recons_{6}x"),
        }
        LOGGER.info(f"üìÅ Data Roots Initialized for {u_method}")

        # === Level 2: Process Patients ===
        process_patients_and_store_stats(
            pat_ids          = pat_ids,
            acc_factors      = acc_factors,
            uq_method        = u_method,
            uq_metric        = uq_metrics[uq_metric],  # or "cv" depending on the method
            roots            = roots,
            db_fpath         = db_fpath,
            table_name       = tablename,
            do_blurring      = do_blurring,
            debug            = debug,
        )